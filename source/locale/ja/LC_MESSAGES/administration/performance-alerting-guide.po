# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2015-2019 Mattermost
# This file is distributed under the same license as the Mattermost package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Mattermost 5.18\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-06-03 20:45+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../source/administration/performance-alerting-guide.rst:2
msgid "Mattermost Performance Alerting Guide"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:4
msgid ""
"Mattermost uses Prometheus and Grafana to track performance metrics of "
"the Mattermost application servers. The purpose of this guide is to help "
"you set up alerts on your Grafana dashboard."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:6
msgid "**Prerequisites**"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:8
msgid ""
"Set up performance monitoring for Mattermost. See our `documentation "
"<https://docs.mattermost.com/deployment/metrics.html>`__ to learn more."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:10
msgid ""
"To get alerts, first set up a Notification Channel in Grafana. Here’s how"
" you can set it up to automatically post alerts in Mattermost:"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:12
msgid "In Mattermost:"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:13
msgid "Create an Alerts channel"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:14
msgid ""
"Create an `incoming webhook <https://docs.mattermost.com/developer"
"/webhooks-incoming.html>`__ for the Alerts channel and copy the URL"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:16
msgid "In Grafana:"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:17
msgid "Under the alert icon in the sidebar, click ''Notification channels''"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:18
msgid "Click ''Add channel''"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:19
msgid "Enter ''Mattermost Alerts Channel'' as the name"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:20
msgid "For type, select ''Slack''"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:21
msgid "Paste your webhook URL into the URL field"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:22
msgid ""
"Include an @ mention in the mention field, if you want to send mentions "
"when an alert is posted to Mattermost"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:23
msgid "Press ''Send Test'' to test the alert"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:25
msgid ""
"If you would also like to get email alerts, you can follow `these "
"instructions <http://docs.grafana.org/alerting/notifications/#email.>`__ "
"to set that up."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:27
msgid "**Configuring Alerts**"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:29
msgid ""
"The `Mattermost dashboards "
"<https://grafana.com/dashboards?search=mattermost>`__ for Grafana come "
"with some partially pre-configured alerts on the following charts:"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:31
#: ../../source/administration/performance-alerting-guide.rst:64
msgid "CPU Utilization Rate"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:32
#: ../../source/administration/performance-alerting-guide.rst:77
msgid "Memory Usage"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:33
#: ../../source/administration/performance-alerting-guide.rst:86
msgid "Number of Goroutines"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:34
#: ../../source/administration/performance-alerting-guide.rst:97
msgid "Number of API Errors per Second"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:35
#: ../../source/administration/performance-alerting-guide.rst:108
msgid "Mean API Request Time"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:37
msgid ""
"To configure alerts, set an appropriate threshold and enable "
"notifications. Enabling notifications is the same for each chart, but "
"setting the correct threshold can have some variances that are better "
"handled on a per-chart basis."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:39
msgid "For each chart, click on the chart name and click ''Edit'':"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:43
msgid "Then click on the ''Alert'' tab:"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:47
msgid ""
"The alert threshold, which will be discussed in the sections below, is "
"the last field under ''Conditions'' (the one set to 600 in the screenshot"
" above)."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:49
msgid ""
"See the sections below for how to set the threshold for each individual "
"chart. If you would like to add your own custom alert conditions, "
"configure them here."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:53
msgid ""
"To enable the notifications for any alerts, click on the ''Notification''"
" tab on the left and select ''Mattermost Alerts Channel'' under ''Send "
"to'':"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:57
msgid "Enter a message if you would like to add more context to the alert."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:59
msgid ""
"By default, the alerts are configured to check the average of a chart "
"over the last minute to see if that value is above a threshold. If it’s "
"above the threshold, the alert will be triggered. Since it’s an average "
"over the last minute, small spikes that go past the threshold won’t "
"necessarily cause an alert. This helps prevent false positives that would"
" result from natural spikes in usage. The alert state for each chart is "
"evaluated every minute."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:61
msgid "The sections below describe each chart in more detail."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:66
#, python-format
msgid ""
"CPU Utilization Rate is fairly straightforward. CPU Utilization Rate "
"tracks the CPU usage of the app servers as a percentage. The maximum "
"percentage is based on the number of CPU cores or vCPUs your app server "
"has. For example, if you have four CPU cores and your app server was at "
"100% utilization rate on all four cores, the graph would show 400% for "
"that app server."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:68
msgid ""
"It’s best to set the alert threshold based on your average CPU "
"utilization and how many cores/vCPUs your app servers have. Take a look "
"at the chart over the last seven days. You’ll want to set the threshold "
"somewhere between your maximum CPU usage (cores * 100) and the CPU usage "
"you see. The lower you set the threshold, the more alerts, and therefore "
"the more false positives, you will get. Set the threshold too high and "
"you may not receive an alert for an incident until it’s gotten worse. The"
" same principle applies to all alerts, regardless of the chart."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:70
msgid "For example, on our community server, we have the threshold set to 15%:"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:74
msgid ""
"This value is below our maximum CPU usage and above our average usage at "
"peak times. Therefore, we will get alerts if we begin experiencing "
"unusually high CPU usage."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:79
msgid ""
"Memory Usage tracks the megabytes of RAM that your app servers are using."
" Set the threshold similar to the CPU Utilization Rate: below maximum "
"available memory and above your average usage during peak times."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:81
msgid "Here’s how we have the alert set on our community server:"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:88
msgid ""
"Goroutines are functions or methods that run concurrently with other "
"functions and methods. Goroutines are like lightweight threads with low-"
"creation costs. A rising number of goroutines can be a good measure of "
"the performance of your app servers. A continuous increase indicates your"
" app server can't keep up and is creating goroutines faster than they can"
" complete their tasks and stop."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:90
msgid ""
"Set the threshold somewhere above the average number of goroutines you "
"see during peak load times. Small spikes are usually nothing to worry "
"about. It’s the uncontrolled climbing of goroutines that you want to "
"watch out for."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:92
msgid "Here’s how we have it set on our community server:"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:99
msgid ""
"Any 4xx or 5xx HTTP response status codes are counted as a REST API "
"error. API errors themselves are not necessarily a problem. There are "
"many legitimate reasons for an API error to occur, such as users’ "
"sessions expiring or clients requesting to see if a resource exists and "
"is being given a ''404 Not Found'' response. It is normal to have some "
"API errors that scale with your installation base."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:101
msgid ""
"That said, errors against the REST API can be indicative of deployment "
"and other issues. For example, if one of your app servers did not deploy "
"correctly for whatever reason, it may begin returning a high number of "
"API errors. Another example would be a rogue bot spamming the API with "
"bad requests. Alerts on API errors per second would help catch these and "
"other issues."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:103
#: ../../source/administration/performance-alerting-guide.rst:114
msgid "Here’s how it’s set on our community server:"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:110
msgid ""
"The Mean API Request Time is the average amount of time a REST API "
"request to the Mattermost app server takes to complete. If an app server "
"starts to perform poorly, you’ll likely see a rise in the mean request "
"time as it takes longer to complete requests. This could also happen if "
"your database can’t sustain the load from the app servers. It may also be"
" indicative of an issue between the app servers and your proxy."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:112
msgid ""
"You’ll want to set the alert threshold a little above what the mean "
"request time is during your peak load times."
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:119
msgid "Other Alerts"
msgstr ""

#: ../../source/administration/performance-alerting-guide.rst:121
msgid ""
"If you want more alerts, you can set them up on any of the Grafana charts"
" you'd like."
msgstr ""

